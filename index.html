<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>
    <script src="tf.min.js"></script>
    <script src="tf-backend-webgpu.js"></script>
  </head>
  <body>
    <h1>本DEMO 可以将人声移除, 上传文件后请耐心等待，直到进度条完成</h1>
    <input type="file" id="file" />
    <progress id="progress" value="0" max="100"></progress
    ><span id="tips"></span>
    <script>
      // Function to convert AudioBuffer to WAV Blob
      function bufferToWavBlob(buffer) {
        const numberOfChannels = 2;
        const sampleRate = 44100 / 2;
        const length = buffer[0].length;

        const arrayBuffer = new ArrayBuffer(44 + length * 2 * numberOfChannels);
        const view = new DataView(arrayBuffer);

        // RIFF identifier
        writeString(view, 0, "RIFF");
        // RIFF chunk length
        view.setUint32(4, 36 + length * 2, true);
        // RIFF type (WAVE)
        writeString(view, 8, "WAVE");
        // format chunk identifier
        writeString(view, 12, "fmt ");
        // format chunk length
        view.setUint32(16, 16, true);
        // sample format (PCM)
        view.setUint16(20, 1, true);
        // channel count
        view.setUint16(22, numberOfChannels, true);
        // sample rate
        view.setUint32(24, sampleRate, true);
        // byte rate (sample rate * block align)
        view.setUint32(28, sampleRate * 2 * numberOfChannels, true);
        // block align (channel count * bytes per sample)
        view.setUint16(32, numberOfChannels * 2, true);
        // bits per sample
        view.setUint16(34, 16, true);
        // data chunk identifier
        writeString(view, 36, "data");
        // data chunk length
        view.setUint32(40, length * 2, true);
        // console.log("view", buffer, buffer?.getChannelData(0));

        // 处理每个通道的数据
        for (let channel = 0; channel < numberOfChannels; channel++) {
          floatTo16BitPCM(view, 44 + channel * length * 2, buffer[channel]);
        }
        return new Blob([view], { type: "audio/wav" });
      }

      function writeString(view, offset, string) {
        for (let i = 0; i < string.length; i++) {
          view.setUint8(offset + i, string.charCodeAt(i));
        }
      }

      function floatTo16BitPCM(output, offset, input) {
        for (let i = 0; i < input.length; i++, offset += 2) {
          const s = Math.max(-1, Math.min(1, input[i]));
          output.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
        }
      }

      async function istft(x) {
        const n = 6144 / 2 + 1;
        const f_pad = tf.zeros([1, 4, 1, 32]);
        let x1 = tf.concat([x, f_pad], x.shape.length - 2);
        f_pad.dispose();
        let x2 = x1.reshape([1, 2, 2, 3073, 32]);
        x1.dispose();
        let x3 = x2.reshape([-1, 2, n, 32]);
        x2.dispose();
        let x4 = x3.transpose([0, 2, 3, 1]);
        x3.dispose();
        const splitTensors = tf.split(x4, 2, 0);
        x4.dispose();
        let l = splitTensors[0];
        let l2 = tf.squeeze(l, [0]);
        let r = splitTensors[1];
        let r2 = tf.squeeze(r, [0]);
        let result = await getIstftLR(l2, r2, 6144, 1024, createHannWindow);

        l.dispose();
        l2.dispose();
        r.dispose();
        r2.dispose();
        return result;
      }

      function getStft(input) {
        return tf.tidy(() => {
          let res = tf.signal.stft(input, 6144, 1024, 6144, createHannWindow);
          let real = tf.real(res);
          let imag = tf.imag(res);
          real = tf.slice(real, [0, 0], [32, 3072]);
          imag = tf.slice(imag, [0, 0], [32, 3072]);
          real = real.transpose();
          imag = imag.transpose();
          let output = tf.stack([real, imag], 0);
          let zeroLike = tf.zerosLike(output);
          let isNaN = tf.isNaN(output);
          return tf.where(isNaN, zeroLike, output);
        });
      }

      async function getIstftLR(
        spectrogramL,
        spectrogramR,
        fftSize,
        hopSize,
        windowFn
      ) {
        let tasks = [];
        let needsToDispose = [];
        const numFrames = spectrogramL.shape[1];
        let size = fftSize + hopSize * (numFrames - 1);
        let outputArrayL = new Float32Array(size);
        let outputArrayR = new Float32Array(size);

        for (let g = 0; g < 2; g++) {
          let spectrogram;
          if (g == 0) {
            spectrogram = spectrogramL;
          } else {
            spectrogram = spectrogramR;
          }

          const numFrames = spectrogram.shape[1];
          const numFrequencyBins = spectrogram.shape[0];
          let window = windowFn ? windowFn(fftSize) : tf.ones([fftSize]);

          for (let i = 0; i < numFrames; i++) {
            let frame = spectrogram.slice([0, i], [numFrequencyBins, 1]);
            let frame2 = tf.squeeze(frame);
            let realPart = frame2.slice([0, 0], [-1, 1]);
            let imagPart = frame2.slice([0, 1], [-1, 1]);
            let complexTensor = tf.complex(realPart, imagPart);
            let complexTensor2 = tf.squeeze(complexTensor);
            let timeDomainFrame = tf.spectral.irfft(complexTensor2);
            let timeDomainFrame2 = tf.mul(timeDomainFrame, window);

            frame.dispose();
            frame2.dispose();
            realPart.dispose();
            imagPart.dispose();
            complexTensor.dispose();
            complexTensor2.dispose();
            timeDomainFrame.dispose();
            needsToDispose.push(timeDomainFrame2);

            tasks.push(timeDomainFrame2.data());
          }

          window.dispose();
        }

        let completedTasks = await Promise.all(tasks);

        for (const iterator of needsToDispose) {
          iterator.dispose();
        }

        let framesL = completedTasks.splice(0, numFrames);
        let framesR = completedTasks;

        for (let i = 0; i < numFrames; i++) {
          const timeDomainArray = framesL[i];
          const startIdx = i * hopSize;
          for (let j = 0; j < fftSize; j++) {
            if (startIdx + j < outputArrayL.length) {
              outputArrayL[startIdx + j] += timeDomainArray[j];
            }
          }
        }
        for (let i = 0; i < numFrames; i++) {
          const timeDomainArray = framesR[i];
          const startIdx = i * hopSize;
          for (let j = 0; j < fftSize; j++) {
            if (startIdx + j < outputArrayR.length) {
              outputArrayR[startIdx + j] += timeDomainArray[j];
            }
          }
        }
        return [outputArrayL, outputArrayR];
      }

      function createHannWindow(length) {
        let ts = tf.tensor(tf.signal.hannWindow(length).dataSync());
        return ts;
      }

      // Create an AudioContext instance
      const audioContext = new (window.AudioContext ||
        window.webkitAudioContext)({
        sampleRate: 44100,
        channelCount: 1,
        echoCancellation: false,
        autoGainControl: false,
        noiseSuppression: false,
      });
      let writtenPointer = 0;

      document.getElementById("file").addEventListener("change", function (e) {
        const file = e.target.files[0];
        const reader = new FileReader();
        reader.onload = function (e) {
          const audioData = e.target.result;
          handler(audioData);
        };
        reader.readAsArrayBuffer(file);
      });

      // Load the audio file using an XMLHttpRequest
      //   const xhr = new XMLHttpRequest();
      //   xhr.open("GET", "./test.mp3", true);
      //   xhr.responseType = "arraybuffer";
      const handler = function a(audioData) {
        // // Decode the audio data
        audioContext.decodeAudioData(audioData, async function (buffer) {
          await tf.setBackend("webgpu");
          const { numberOfChannels, sampleRate } = buffer;
          // offlineAudioContext相对AudioContext更加节省资源
          const offlineAudioContext = new OfflineAudioContext(
            numberOfChannels,
            buffer.length,
            sampleRate
          );
          var source = offlineAudioContext.createBufferSource();
          source.buffer = buffer;
          source.connect(offlineAudioContext.destination);
          source.start();
          // 处理不同声道的音频信息 1.如果是单声道， 需要克隆一份声道数据 2. 如果是双声道，则直接使用
          const channelData =
            numberOfChannels === 1
              ? [
                  Array.from(buffer.getChannelData(0)),
                  Array.from(buffer.getChannelData(0)),
                ]
              : [
                  Array.from(buffer.getChannelData(0)),
                  Array.from(buffer.getChannelData(1)),
                ];
          const model = await tf.loadGraphModel("./models/model.json");

          const chunkSize = 31744;
          //  根据chunkSize进行分块处理 channelData
          const chunkedChannelData = [];

          for (let i = 0; i < channelData[0].length; i += chunkSize) {
            const chunk = [
              channelData[0].slice(i, i + chunkSize),
              channelData[1].slice(i, i + chunkSize),
            ];
            chunkedChannelData.push(chunk);
          }
          console.log("buffer0", chunkedChannelData);
          const writeBuffers0 = [];
          const writeBuffers1 = [];
          const writeVocalBuffers0 = [];
          const writeVocalBuffers1 = [];
          let progress = 0;
          for await (let chunk of chunkedChannelData) {
            const $channelData = chunk;
            const runner = async () => {
              tf.engine().startScope();
              const [buffer0, buffer1] = $channelData;
              if (buffer0.length < 31744) {
                return;
              }
              let totalBufferPointer = buffer0.length;
              let currentLast = totalBufferPointer;

              let originalLeft = buffer0.slice(
                buffer0.length - 31744,
                buffer0.length
              );
              let originalRight = buffer1.slice(
                buffer1.length - 31744,
                buffer1.length
              );

              let left = new Array(3072)
                .fill(0)
                .concat(originalLeft)
                .concat(new Array(3072).fill(0));
              let right;
              if (buffer1.length >= 31744) {
                right = new Array(3072)
                  .fill(0)
                  .concat(originalRight)
                  .concat(new Array(3072).fill(0));
              } else {
                right = left.concat();
              }

              buffer0.splice(0, buffer0.length - 31744 - 1);
              buffer1.splice(0, buffer1.length - 31744 - 1);

              let inputL = tf.tensor1d(left);
              let inputR = tf.tensor1d(right);

              let l = getStft(inputL);
              let r = getStft(inputR);

              inputL.dispose();
              inputR.dispose();

              let t0 = tf.stack([l, r], 3);
              l.dispose();
              r.dispose();
              let t1 = t0.transpose([0, 3, 1, 2]);
              t0.dispose();
              let reshaped = t1.reshape([
                1,
                2,
                2,
                -1,
                t1.shape[t1.shape.length - 1],
              ]);
              let reshaped2 = reshaped.reshape([
                1,
                4,
                -1,
                t1.shape[t1.shape.length - 1],
              ]);
              t1.dispose();
              reshaped.dispose();
              let data = tf.tidy(() => {
                return model.predict(reshaped2);
              });
              let vocaldata = tf.tidy(() => {
                return tf.sub(reshaped2, data);
              });
              reshaped2.dispose();
              let signals = await istft(data);
              let signals2 = await istft(vocaldata);
              tf.engine().endScope();
              let elapse = currentLast - writtenPointer;
              let outdata0 = Array.from(signals[0]);
              outdata0 = outdata0.splice(3072, 31744);
              let outdata1 = Array.from(signals[1]);
              outdata1 = outdata1.splice(3072, 31744);
              let writeBuffer0 = [];
              let writeBuffer1 = [];
              if (elapse < outdata0.length) {
                writeBuffer0 = writeBuffer0.concat(
                  outdata0.slice(outdata0.length - elapse, outdata0.length)
                );
              } else {
                writeBuffer0 = writeBuffer0.concat(outdata0);
              }

              if (elapse < outdata0.length) {
                writeBuffer1 = writeBuffer1.concat(
                  outdata1.slice(outdata1.length - elapse, outdata1.length)
                );
              } else {
                writeBuffer1 = writeBuffer1.concat(outdata1);
              }
              writeBuffers0.push(...writeBuffer0);
              writeBuffers1.push(...writeBuffer1);
              outdata0 = Array.from(signals2[0]);
              outdata0 = outdata0.splice(3072, 31744);
              outdata1 = Array.from(signals2[1]);
              outdata1 = outdata1.splice(3072, 31744);
              writeBuffer0 = [];
              writeBuffer1 = [];
              if (elapse < outdata0.length) {
                writeBuffer0 = writeBuffer0.concat(
                  outdata0.slice(outdata0.length - elapse, outdata0.length)
                );
              } else {
                writeBuffer0 = writeBuffer0.concat(outdata0);
              }

              if (elapse < outdata0.length) {
                writeBuffer1 = writeBuffer1.concat(
                  outdata1.slice(outdata1.length - elapse, outdata1.length)
                );
              } else {
                writeBuffer1 = writeBuffer1.concat(outdata1);
              }
              writeVocalBuffers0.push(...writeBuffer0);
              writeVocalBuffers1.push(...writeBuffer1);
              tf.dispose(vocaldata);
              tf.dispose(data);
            };
            await runner();
            progress += 1;
            document.getElementById("progress").value = Math.floor(
              (progress * 100) / chunkedChannelData.length
            );
            document.getElementById("tips").innerText = `进度${Math.floor(
              (progress * 100) / chunkedChannelData.length
            )}%`;
            // console.log("progress", (progress += 1), chunkedChannelData.length);
          }
          // 测试时解开
          const blob = bufferToWavBlob([writeBuffers0, writeBuffers1]);
          // const blob = bufferToWavBlob(buffer);
          // Create a download link
          const a = document.createElement("a");
          document.body.appendChild(a);
          a.style = "display: none";

          // Create a URL for the Blob and set it as the href attribute
          const url = URL.createObjectURL(blob);
          a.href = url;
          a.download = "background-music.wav";
          const vblob = bufferToWavBlob([writeVocalBuffers0, writeVocalBuffers1]);
          // const blob = bufferToWavBlob(buffer);
          // Create a download link
          const va = document.createElement("a");
          document.body.appendChild(va);
          va.style = "display: none";

          // Create a URL for the Blob and set it as the href attribute
          const vurl = URL.createObjectURL(vblob);
          va.href = vurl;
          va.download = "vocal-music.wav";

          // Trigger a click on the link to start the download
          a.click();
          va.click();

          // Clean up the URL object
          window.URL.revokeObjectURL(url);
          window.URL.revokeObjectURL(vurl);
        });
      };

      xhr.send();
    </script>
  </body>
</html>
